{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1QkHCYGLtLKS1o7xY3c5AAPoENXEezu2k",
      "authorship_tag": "ABX9TyP+hekqytDTFDBDmSMW5sMb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hoso1000/kaken-kubun-finder/blob/main/kaken_kubun_finder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 科研費区分ファインダー（仮）\n",
        "\n",
        "応募しようとする科研費の課題名＋概要をもとに、すでに採択されている科研費課題（KAKENデータベースに掲載されているもの）から課題名・概要が似ている課題を探し出し、似ている課題がどの小区分で多く採択されているかチェックできます。"
      ],
      "metadata": {
        "id": "1T0Na1pg5N3L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Google Driveをマウント\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "6d5w2BWIN8zz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 必要なライブラリ等をインストール\n",
        "!pip install -q transformers==4.7.0 fugashi ipadic\n",
        "!aptitude install mecab libmecab-dev mecab-ipadic-utf8 git make curl xz-utils file -y\n",
        "!pip install mecab-python3==0.996.5\n",
        "!pip install transformers"
      ],
      "metadata": {
        "id": "LC_AYMDZ_QHt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertJapaneseTokenizer, BertModel\n",
        "import torch\n",
        "\n",
        "# 日本語Sentence-BERTクラス定義\n",
        "class SentenceBertJapanese:\n",
        "    def __init__(self, model_name_or_path, device=None):\n",
        "        self.tokenizer = BertJapaneseTokenizer.from_pretrained(model_name_or_path)\n",
        "        self.model = BertModel.from_pretrained(model_name_or_path)\n",
        "        self.model.eval()\n",
        "\n",
        "        if device is None:\n",
        "            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.device = torch.device(device)\n",
        "        self.model.to(device)\n",
        "\n",
        "    def _mean_pooling(self, model_output, attention_mask):\n",
        "        token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
        "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def encode(self, sentences, batch_size=8):\n",
        "        all_embeddings = []\n",
        "        iterator = range(0, len(sentences), batch_size)\n",
        "        for batch_idx in iterator:\n",
        "            batch = sentences[batch_idx:batch_idx + batch_size]\n",
        "\n",
        "            encoded_input = self.tokenizer.batch_encode_plus(batch, padding=\"longest\", \n",
        "                                           truncation=True, return_tensors=\"pt\").to(self.device)\n",
        "            model_output = self.model(**encoded_input)\n",
        "            sentence_embeddings = self._mean_pooling(model_output, encoded_input[\"attention_mask\"]).to('cpu')\n",
        "\n",
        "            all_embeddings.extend(sentence_embeddings)\n",
        "\n",
        "        # return torch.stack(all_embeddings).numpy()\n",
        "        return torch.stack(all_embeddings)"
      ],
      "metadata": {
        "id": "vX8Cro1z77Se"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 日本語モデルの読み込み\n",
        "model = SentenceBertJapanese(\"sonoisa/sentence-bert-base-ja-mean-tokens-v2\")"
      ],
      "metadata": {
        "id": "uzx-2oIHYmDx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# KAKEN DB採択課題データを読み込み、データフレームkakenに格納\n",
        "import pandas as pd\n",
        "path = '/content/drive/MyDrive/kaken-kubun-finder/'\n",
        "# KAKEN DB採択課題はkaken.nii.ac.jp_BCW.csvとして上記フォルダにダウンロードしておく\n",
        "# （このフォルダ内のkaken.nii.ac.jp_BCW.csvは2018-2022年度開始の基盤B・C・若手の全課題を2022年10月にDLしたもの）\n",
        "kaken = pd.read_csv(path + 'kaken.nii.ac.jp_BCW.csv') \n",
        "\n",
        "# kakenの研究課題名・概要が空欄の課題を削除\n",
        "kaken = kaken[(kaken['研究課題名'].isna()==False)&(kaken['研究開始時の研究の概要'].isna()==False)].reset_index(drop = True)\n",
        "\n",
        "# 英文の課題を削除し、インデックスを振り直す\n",
        "l_drop = []\n",
        "for i in kaken.index:\n",
        "  kadai_i = kaken.iat[i, 1]\n",
        "  gaiyo_i = kaken.iat[i, 27]\n",
        "  if len((kadai_i).encode())/len(kadai_i) < 1.5:\n",
        "    l_drop.append(i)\n",
        "  elif len((gaiyo_i).encode())/len(gaiyo_i) < 1.5:\n",
        "    l_drop.append(i)\n",
        "  else: continue\n",
        "\n",
        "kaken = kaken.drop(kaken.index[l_drop]).reset_index(drop = True)"
      ],
      "metadata": {
        "id": "JIx4UNPE8BYK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 採択課題の課題名の文ベクトルを計算して sentence-vector-v2.ptを作成\n",
        "sentences = []\n",
        "for i in kaken.index:\n",
        "  gaiyo = kaken.at[i, '研究開始時の研究の概要']\n",
        "  gaiyo = gaiyo + '：' + kaken.at[i,'研究課題名']\n",
        "  sentences.append(gaiyo)\n",
        "\n",
        "# sentencesの文ベクトルを計算してファイル保存\n",
        "sentence_vectors = model.encode(sentences)\n",
        "torch.save(sentence_vectors, path + 'sentence-vector-v2.pt')"
      ],
      "metadata": {
        "id": "c3b_jTxgtG2X"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 近い区分を調べたい課題の研究課題名を入力する\n",
        "qkadai = input('研究課題名を入力: ')\n",
        "# 近い区分を調べたい課題の研究概要を入力する\n",
        "qgaiyo = input('研究概要を入力: ')\n",
        "# 最も似ている課題名から何件取り出すかclosest_nに入力する、指定がなければ10件\n",
        "n = input('何件取り出す？半角数字で入力：')\n",
        "try:\n",
        "    closest_n = int(n)\n",
        "except ValueError:\n",
        "    closest_n = 10"
      ],
      "metadata": {
        "id": "MqKvV-Ri8Gnk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "#queryに概要・課題名を格納する\n",
        "query = re.sub('．','。',qgaiyo)\n",
        "query = query.split('。')\n",
        "query.append(qkadai)\n",
        "# queryについて文ベクトルを計算し、query_embeddingsに格納する\n",
        "query_embeddings = model.encode(query).numpy()"
      ],
      "metadata": {
        "id": "C7MFEIcGagqu"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.spatial\n",
        "\n",
        "# 採択課題の文ベクトルsentence-vector-v2.ptを読み出す\n",
        "sentence_vectors = torch.load(path + 'sentence-vector-v2.pt')\n",
        "\n",
        "# 各データを格納するリストを作成する\n",
        "r_query = [] #応募課題名\n",
        "r_result1 = [] #似ている課題\n",
        "r_result2 =[] #似ている課題の審査区分\n",
        "r_result3 =[] #似ている課題の研究の概要\n",
        "r_distance = [] #コサイン距離\n",
        "\n",
        "# 応募課題の文ベクトルquery_embeddingsと採択課題の文ベクトルsentence_vectorsとのコサイン距離を求め、distancesに格納する\n",
        "distances = scipy.spatial.distance.cdist(query_embeddings, sentence_vectors, metric=\"cosine\")[0]\n",
        "\n",
        "# コサイン距離distancesについて小さい順にソートして辞書resultsに格納\n",
        "results = zip(range(len(distances)), distances)\n",
        "results = sorted(results, key=lambda x: x[1])\n",
        "\n",
        "print(qkadai + '\\n==========\\n')\n",
        "# resultsが小さかったもの上位closest_n位について\n",
        "for idx, distance in results[0:closest_n]:\n",
        "   # 課題名が似ている課題名、採択課題と応募課題とのコサイン距離、似ている課題の審査区分, 研究の概要を格納する\n",
        "    try:\n",
        "      r_query.append(qkadai) # 応募課題\n",
        "      r_distance.append(distance / 2) # 採択課題と応募課題とのコサイン距離\n",
        "      r_result1.append(kaken.iat[idx,1]) # 似ている課題\n",
        "      r_result2.append(kaken.iat[idx,14]) # 似ている課題の審査区分\n",
        "      r_result3.append(kaken.iat[idx,27]) #似ている課題の研究の概要\n",
        "      print(kaken.iat[idx,1], \"(Score: %.4f)\" % (distance / 2), kaken.iat[idx,14])\n",
        "    except Exception as e:\n",
        "      print(e)\n",
        "    else: continue"
      ],
      "metadata": {
        "id": "Tw8l8WHu8KpP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 結果をcsvファイルとしてresultフォルダ内に保存する\n",
        "import datetime\n",
        "dt_now = datetime.datetime.now()\n",
        "\n",
        "# 結果格納用のDataFrame df_rを作成する\n",
        "df_r = pd.DataFrame()\n",
        "df_r['研究課題名(和文)'] = r_query\n",
        "df_r['似ている課題名'] = r_result1\n",
        "df_r['審査区分'] = r_result2\n",
        "df_r['コサイン距離'] = r_distance\n",
        "\n",
        "df_r.to_csv(path + 'result/kaken-kubun-finder-result-' + dt_now.strftime('%Y-%m-%d_%H-%M-%S')  + '.csv', encoding = 'cp932')"
      ],
      "metadata": {
        "id": "zP-0KIum8LYk"
      },
      "execution_count": 14,
      "outputs": []
    }
  ]
}